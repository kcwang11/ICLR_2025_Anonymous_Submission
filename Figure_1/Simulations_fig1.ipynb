{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6fb3f5",
   "metadata": {},
   "source": [
    "# Simulations for Figure 1\n",
    "This file contains the gpytorch simulations for figure 1. To run this file, set the \"gpu\" variable below to true if you would like to use Cuda or false if you would not. Then, simply run each of the cells. It will output two csv files that the HS-SVD-fig1.Rmd file will use to create the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de00538",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "import faiss\n",
    "from matplotlib import pyplot as plt\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "# Make plots inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import psutil\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    "\n",
    "\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d29fd88-72ae-43bf-9292-e9f77f51d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "\n",
    "data_files_x = ['Data/train_x_20k.csv','Data/train_x_31k.csv','Data/train_x_50k.csv','Data/train_x_80k.csv','Data/train_x_120k.csv','Data/train_x_200k.csv']\n",
    "data_files_y = ['Data/train_y_20k.csv','Data/train_y_31k.csv','Data/train_y_50k.csv','Data/train_y_80k.csv','Data/train_y_120k.csv','Data/train_y_200k.csv']\n",
    "\n",
    "\n",
    "csvfile = pd.read_csv('Data/test_x_200k.csv', header = None)\n",
    "test_x = torch.tensor(csvfile[0]).float()\n",
    "csvfile = pd.read_csv('Data/test_y_200k.csv', header = None)\n",
    "test_y = torch.tensor(csvfile[0]).float()\n",
    "\n",
    "\n",
    "test_x = test_x.contiguous()\n",
    "test_y = test_y.contiguous()\n",
    "\n",
    "\n",
    "\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955df80-1c9d-4055-8e5e-3ea753501029",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# SKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccb967-8318-4395-8578-2b7a3d2105c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_ski = []\n",
    "\n",
    "for i in np.arange(0,6):\n",
    "    csvfile = pd.read_csv(data_files_x[i], header = None)\n",
    "    train_x = torch.tensor(csvfile[0]).float()\n",
    "    train_x = torch.reshape(train_x,[-1,1])\n",
    "    csvfile = pd.read_csv(data_files_y[i], header = None)\n",
    "    train_y = torch.tensor(csvfile[0]).float()\n",
    "    train_n = int(len(train_x))\n",
    "    train_x = train_x.contiguous()\n",
    "    train_y = train_y.contiguous()\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "    print(test_x.shape)\n",
    "    print(test_y.shape)\n",
    "    if gpu:\n",
    "        train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "    class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "            # SKI requires a grid size hyperparameter. This util can help with that. Here we are using a grid that has the same number of points as the training data (a ratio of 1.0). Performance can be sensitive to this parameter, so you may want to adjust it for your own problem on a validation set.\n",
    "            grid_size = gpytorch.utils.grid.choose_grid_size(train_x,1.0/25.0)\n",
    "\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.GridInterpolationKernel(\n",
    "                    gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1), grid_size=grid_size, num_dims=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "    my_batch_size = 320\n",
    "    smoke_test = False\n",
    "\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "    # this is for running the notebook in our testing framework\n",
    "    import os\n",
    "    smoke_test = ('CI' in os.environ)\n",
    "    training_iterations = 32\n",
    "\n",
    "    mem_begin = get_mem()\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    begin = time.time()\n",
    "\n",
    "    for i in tqdm.tqdm(range(training_iterations), desc=\"Train\"):\n",
    "        optimizer.zero_grad()\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "    # print(time.time()-begin)\n",
    "    # print(\"RAM: \",(get_mem() - mem_begin)/(1024**2))\n",
    "    # print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "    time_ski.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    mll = None\n",
    "    train_x = None\n",
    "    train_y = None\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "time_ski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dbdca4",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf6cf",
   "metadata": {},
   "source": [
    "# Lanczos Variance Estimates (LOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed2886-f6e2-42b7-b5c8-72a61f4bc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_love = []\n",
    "\n",
    "for i in np.arange(0,6):\n",
    "    csvfile = pd.read_csv(data_files_x[i], header = None)\n",
    "    train_x = torch.tensor(csvfile[0]).float()\n",
    "    train_x = torch.reshape(train_x,[-1,1])\n",
    "    csvfile = pd.read_csv(data_files_y[i], header = None)\n",
    "    train_y = torch.tensor(csvfile[0]).float()\n",
    "    train_n = int(len(train_x))\n",
    "    train_x = train_x.contiguous()\n",
    "    train_y = train_y.contiguous()\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "    print(test_x.shape)\n",
    "    print(test_y.shape)\n",
    "    if gpu:\n",
    "        train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "\n",
    "    my_batch_size = 3200\n",
    "    smoke_test = False\n",
    "\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "    class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "        def __init__(self, input_dim):\n",
    "            super(LargeFeatureExtractor, self).__init__()\n",
    "            self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "            self.add_module('relu1', torch.nn.ReLU())\n",
    "            self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "            self.add_module('relu2', torch.nn.ReLU())\n",
    "            self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "            self.add_module('relu3', torch.nn.ReLU())\n",
    "            self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "            print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "            \n",
    "\n",
    "\n",
    "    class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)),\n",
    "                grid_size=100, num_dims=1,\n",
    "            )\n",
    "\n",
    "            # Also add the deep net\n",
    "            self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            # The rest of this looks like what we've seen\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            vram_usage()\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "\n",
    "    # 2k, 20k, 200k\n",
    "    # 30, \n",
    "    training_iterations = 30 #\n",
    "\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations))\n",
    "        for i in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "\n",
    "    #%time train()\n",
    "\n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    uRAM = (get_mem() - mem_begin)/(1024**2)\n",
    "    uVRAM = max_vram / (1024 ** 2)\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    time_love.append(uTime)\n",
    "    print('SKI: MSE &Time &RAM &VRAM: {}'.format(MSE), \"&\", round(uTime,2), \"&\", round(uRAM,2) , \"&\" , round(uVRAM,2) )\n",
    "    likelihood = None\n",
    "    model = None\n",
    "    mll = None\n",
    "    train_x = None\n",
    "    train_y = None\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "time_ski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f3d8a",
   "metadata": {},
   "source": [
    "# Outputing csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e43d83-1b30-4d8b-8058-cf4c8d840b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('time_ski.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    for item in time_ski:\n",
    "        writer.writerow([item])\n",
    "\n",
    "\n",
    "with open('time_love.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for item in time_love:\n",
    "        writer.writerow([item])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaleGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
