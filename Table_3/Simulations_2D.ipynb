{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416b0d9a",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "RAM and VRAM measurements are dependent on the computer state, and should only be interpreted relative to each other. In order to obtain RAM and VRAM measurements, perform the following steps:\n",
    "\n",
    "1 - Restart the Kernel\n",
    "\n",
    "2 - Run the \"Loading Required Packages and Helper Functions\" cell\n",
    "\n",
    "3 - Run the \"Loading Data\" cell\n",
    "\n",
    "4 - Run ONLY ONE iteration of the desired method, and read the RAM and VRAM usage reports printed by the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6bfc9",
   "metadata": {},
   "source": [
    "# Loading Required Packages and Helper Functions\n",
    "If you would like to use Cuda, set gpu = True. Otherwise set gpu = False. \n",
    "\n",
    "Step 1: Run the following cell to import the required packages and helper functions. Set the number of replicates desired.\n",
    "\n",
    "Step 2: Load the Data\n",
    "\n",
    "Step 3: Execute the cells under the method you wish to replicate.\n",
    "\n",
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a4ade5-38b2-4f05-8626-7fb74d9fe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True\n",
    "n_replicates = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6605e5-f791-44f8-ada3-9efb2237ed1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import psutil\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "# Make plots inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def get_mem():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss\n",
    "\n",
    "max_vram = 0\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL\n",
    "\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "max_vram = 0\n",
    "max_ram = 0\n",
    "\n",
    "def vram_usage():\n",
    "    global max_vram\n",
    "    max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "import faiss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e28d-807c-40e4-8868-4146cd1c5911",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Step 2: Load the data (note: must run the DataGenerator.Rmd file first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f66626-c1c8-4f7f-a55d-29b5b743d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(get_mem()/(1024**2))\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "csvfile = pd.read_csv('train_x_2d.csv', header = None, dtype=float, delimiter=' ')\n",
    "train_x = torch.tensor(np.array(csvfile)).float()\n",
    "#train_x = torch.reshape(train_x,[80000,2])\n",
    "csvfile = pd.read_csv('train_y_2d.csv', header = None)\n",
    "train_y = torch.tensor(csvfile[0]).float()\n",
    "csvfile = pd.read_csv('test_x_2d.csv', header = None, dtype=float, delimiter=' ')\n",
    "test_x = torch.tensor(np.array(csvfile)).float()\n",
    "csvfile = pd.read_csv('test_y_2d.csv', header = None)\n",
    "test_y = torch.tensor(csvfile[0]).float()\n",
    "\n",
    "\n",
    "train_n = int(len(train_x))\n",
    "train_x = train_x.contiguous()\n",
    "train_y = train_y.contiguous()\n",
    "\n",
    "test_x = test_x.contiguous()\n",
    "test_y = test_y.contiguous()\n",
    "\n",
    "if gpu:\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6d278-66bf-4276-83bc-946cab62602a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_x.size(-1))\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f697e43",
   "metadata": {},
   "source": [
    "# Simulations\n",
    "Step 3: Execute the simulations to be reproduced. If all simulations are run, there is a summarizer at the end. Otherwise, the relevant statistics are printed at the end of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db075e72",
   "metadata": {},
   "source": [
    "# Deep Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 32\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "data_dim = train_x.size(-1)\n",
    "print(data_dim)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(  gpytorch.kernels.MaternKernel(nu=1.5)  ),\n",
    "                num_dims=1, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            vram_usage()\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "mse_l_dkl = []\n",
    "time_l_dkl = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    training_iterations = 60\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.feature_extractor.parameters()},\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.02)# 0.02 for 100k, 0.02 of Dense\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations), leave = True)\n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(train_x)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    begin=time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    print(uTime)\n",
    "    #%time train()\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    print(\"Memory Usage:\", mem_diff / (1024 ** 2), \"MB\")\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_dkl.append(MSE.item())\n",
    "    time_l_dkl.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    \n",
    "\n",
    "# print(statistics.mean(mse_l_dkl))\n",
    "# print(statistics.stdev(mse_l_dkl))\n",
    "\n",
    "# print(statistics.mean(time_l_dkl))\n",
    "# print(statistics.stdev(time_l_dkl))\n",
    "\n",
    "print(round(statistics.mean(mse_l_dkl),5),round(statistics.stdev(mse_l_dkl),5), round(statistics.mean(time_l_dkl),5), round(statistics.stdev(time_l_dkl),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae2815-2c3a-4ea5-85bc-e7d80e7c52c9",
   "metadata": {},
   "source": [
    "# Sparse GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe2fcd-83d3-4459-b120-9fe92575fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2))\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[::300].clone(), likelihood=likelihood)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "my_batch_size = 320\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "mse_l_sgpr = []\n",
    "time_l_sgpr = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    \n",
    "    training_iterations = 1#350\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "    \n",
    "        for i in iterator:\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Get output from model\n",
    "            output = model(train_x)\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            optimizer.step()\n",
    "            vram_usage()#(torch.cuda.memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    #%time train()\n",
    "    \n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    \n",
    "    #means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_sgpr.append(MSE.item())\n",
    "    time_l_sgpr.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "# print(\"Time: \", time.time() - begin)\n",
    "# print(\"RAM: \", (get_mem() - mem_begin) / (1024 ** 2))\n",
    "# print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "print(statistics.mean(mse_l_sgpr))\n",
    "print(statistics.stdev(mse_l_sgpr))\n",
    "\n",
    "print(statistics.mean(time_l_sgpr))\n",
    "print(statistics.stdev(time_l_sgpr))\n",
    "\n",
    "print(round(statistics.mean(mse_l_sgpr),5),round(statistics.stdev(mse_l_sgpr),5), round(statistics.mean(time_l_sgpr),5), round(statistics.stdev(time_l_sgpr),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e92cd-510c-4dd4-8577-bf9ad4d0dfe7",
   "metadata": {},
   "source": [
    "# LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91503e-88f6-4084-adfe-79aadc7fb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "smoke_test = False\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(input_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 1))\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        \n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)),\n",
    "            grid_size=100, num_dims=1,\n",
    "        )\n",
    "\n",
    "        # Also add the deep net\n",
    "        self.feature_extractor = LargeFeatureExtractor(input_dim=train_x.size(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We're first putting our data through a deep net (feature extractor)\n",
    "        # We're also scaling the features so that they're nice values\n",
    "        projected_x = self.feature_extractor(x)\n",
    "        projected_x = projected_x - projected_x.min(0)[0]\n",
    "        projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "        # The rest of this looks like what we've seen\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mse_l_love = []\n",
    "time_l_love = []\n",
    "\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    training_iterations = 40\n",
    "    \n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    \n",
    "    def train():\n",
    "        iterator = tqdm.tqdm(range(training_iterations))\n",
    "        for i in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    \n",
    "    #%time train()\n",
    "    \n",
    "    begin = time.time()\n",
    "    train()\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "    \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_love.append(MSE.item())\n",
    "    time_l_love.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "print(statistics.mean(mse_l_love))\n",
    "print(statistics.stdev(mse_l_love))\n",
    "\n",
    "print(statistics.mean(time_l_love))\n",
    "print(statistics.stdev(time_l_love))\n",
    "\n",
    "print(round(statistics.mean(mse_l_love),5),round(statistics.stdev(mse_l_love),5), round(statistics.mean(time_l_love),5), round(statistics.stdev(time_l_love),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c952be-27ac-4038-99eb-e670ec4b8b34",
   "metadata": {},
   "source": [
    "# NGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeed018-ae3b-4b7d-a1cb-990009d4eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 320\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_ngd = []\n",
    "time_l_ngd = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    inducing_points = train_x[::100]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    #variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.00001)#0.001 for 100k, 0.001 for Dense\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.01)#0.001 for 100k, 0.001 for Dense\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.1) #0.1 for 100k, 0.1 for Dense\n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    #mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "    num_epochs = 5#15\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    begin = time.time()\n",
    "    \n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            ### Perform NGD step to optimize variational parameters\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            hyperparameter_optimizer.step()\n",
    "    \n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \",time.time()-begin)\n",
    "    mem_diff = get_mem()-mem_begin\n",
    "    print(\"Memory Usage:\", (mem_diff) / (1024 ** 2), \"MB\")\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_ngd.append(MSE.item())\n",
    "    time_l_ngd.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "\n",
    "print(statistics.mean(mse_l_ngd))\n",
    "print(statistics.stdev(mse_l_ngd))\n",
    "\n",
    "print(statistics.mean(time_l_ngd))\n",
    "print(statistics.stdev(time_l_ngd))\n",
    "\n",
    "print(round(statistics.mean(mse_l_ngd),5),round(statistics.stdev(mse_l_ngd),5), round(statistics.mean(time_l_ngd),5), round(statistics.stdev(time_l_ngd),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a445a-b249-48da-be6e-d5937da43568",
   "metadata": {},
   "source": [
    "# SVGP_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9020718-e748-44ac-b8e4-7f5813f65419",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "#inducing_points = train_x[torch.randperm(train_x.size(0))[:200]]\n",
    "inducing_points = train_x[::1000]\n",
    "\n",
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.CiqVariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=2)\n",
    "        )\n",
    "        self.covar_module.base_kernel.initialize(lengthscale=0.01)  # Specific to the 3droad dataset\n",
    "        print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "mse_l_svgpci = []\n",
    "time_l_svgpci = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.variational_parameters(), num_data=train_y.size(0), lr=0.1)\n",
    "    \n",
    "    hyperparameter_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.002) #0.01 for 100k, 0.002 for Dense\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    for i in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "    \n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            hyperparameter_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "            vram_usage()\n",
    "            hyperparameter_optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time()-begin)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "    \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_svgpci.append(MSE.item())\n",
    "    time_l_svgpci.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "print(statistics.mean(mse_l_svgpci))\n",
    "print(statistics.stdev(mse_l_svgpci))\n",
    "\n",
    "print(statistics.mean(time_l_svgpci))\n",
    "print(statistics.stdev(time_l_svgpci))\n",
    "\n",
    "print(round(statistics.mean(mse_l_svgpci),5),round(statistics.stdev(mse_l_svgpci),5), round(statistics.mean(time_l_svgpci),5), round(statistics.stdev(time_l_svgpci),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ac0a0-1575-4237-852e-385f23a4f791",
   "metadata": {},
   "source": [
    "# SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d8af0-7edb-4c15-80a3-e6eb62eda014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_batch_size = 3200\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        #print(\"RAM: \", (get_mem() - mem_begin) / (1024 ** 2))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(\"VRAM Usage:\", torch.cuda.memory_allocated()/(1024**2) , \"MB\")\n",
    "        #print(\"RAM: \", (get_mem() - mem_begin) / (1024 ** 2))\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "mse_l_svgp = []\n",
    "time_l_svgp = []\n",
    "\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    print(\"Replicate:\" ,i)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    inducing_points = train_x[::100]\n",
    "    #inducing_points = train_x\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    #num_epochs = 3# if smoke_test else 4\n",
    "    num_epochs = 20\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.001)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD([\n",
    "    #    {'params': model.parameters()},\n",
    "    #    {'params': likelihood.parameters()},\n",
    "    #], lr=1)\n",
    "    \n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", mem_diff / (1024 ** 2))\n",
    "    \n",
    "    \n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    begin = time.time()\n",
    "    #for i in epochs_iter:\n",
    "    for i in tqdm.tqdm(range(num_epochs), leave = False, position = 0):\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        minibatch_iter = tqdm.tqdm(train_loader, desc=\"Minibatch\", leave=False, position = 0)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "        #for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            loss.backward()\n",
    "            max_ram = max(max_ram, (get_mem() - mem_begin))\n",
    "            optimizer.step()\n",
    "            if gpu:\n",
    "                max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, num_epochs, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            likelihood.noise.item()\n",
    "        ))\n",
    "    uTime = time.time()-begin\n",
    "    print(\"Time: \", time.time() - begin)\n",
    "    mem_diff = get_mem() - mem_begin\n",
    "    print(\"RAM: \", max_ram / (1024 ** 2))\n",
    "    print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    # with torch.no_grad():\n",
    "    #     for x_batch, y_batch in tqdm.tqdm(test_loader):\n",
    "    #         preds = model(x_batch.cuda())\n",
    "    #         means = torch.cat([means, preds.mean.cpu()])\n",
    "     \n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    #    test_x = torch.linspace(0, 1, 51)\n",
    "        if gpu:\n",
    "            observed_pred = likelihood(model(test_x.to('cuda')))\n",
    "        else:\n",
    "            observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    means = observed_pred.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_svgp.append(MSE.item())\n",
    "    time_l_svgp.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "print(statistics.mean(mse_l_svgp))\n",
    "print(statistics.stdev(mse_l_svgp))\n",
    "\n",
    "print(statistics.mean(time_l_svgp))\n",
    "print(statistics.stdev(time_l_svgp))\n",
    "\n",
    "print(round(statistics.mean(mse_l_svgp),5),round(statistics.stdev(mse_l_svgp),5), round(statistics.mean(time_l_svgp),5), round(statistics.stdev(time_l_svgp),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4ccda-39fa-4f20-8b0c-eb815d86c99f",
   "metadata": {},
   "source": [
    "# SKI - Can only handle up to 40,000 datapoints before running out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85087eb9-8b3b-4dad-9125-4ebefd372189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = None\n",
    "likelihood = None\n",
    "\n",
    "if gpu:\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb226a15-2d04-4e1c-9e88-1733fb104f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ski = train_x[::2]\n",
    "train_y_ski = train_y[::2]\n",
    "\n",
    "if gpu:\n",
    "    train_x_ski, train_y_ski = train_x_ski.cuda(), train_y_ski.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24869027-c361-400b-a950-33cdab17cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # SKI requires a grid size hyperparameter. This util can help with that\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x, 1) #1/50\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.RBFKernel(), grid_size=grid_size, num_dims=2\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iterations = 32\n",
    "\n",
    "\n",
    "mse_l_ski = []\n",
    "time_l_ski = []\n",
    "\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x_ski, train_y_ski, likelihood)\n",
    "    \n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  #0.001 Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    if gpu:\n",
    "        mll = mll.cuda()\n",
    "\n",
    "\n",
    "    # # Find optimal model hyperparameters\n",
    "    # model.train()\n",
    "    # likelihood.train()\n",
    "    \n",
    "    # # Use the adam optimizer\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters\n",
    "    \n",
    "    # # \"Loss\" for GPs - the marginal log likelihood\n",
    "    # mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    training_iterations = 15\n",
    "    begin = time.time()\n",
    "    \n",
    "    for i in tqdm.tqdm(range(training_iterations), desc=\"Train\", leave = False, position = 0 ):\n",
    "        optimizer.zero_grad()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        output = model(train_x_ski)\n",
    "        loss = -mll(output, train_y_ski)\n",
    "        loss.backward()\n",
    "        if gpu:\n",
    "            max_vram = max(max_vram, torch.cuda.memory_allocated())\n",
    "        optimizer.step()\n",
    "\n",
    "    uTime = time.time()-begin\n",
    "    print(time.time()-begin)\n",
    "    print(\"RAM: \",(get_mem() - mem_begin)/(1024**2))\n",
    "    print(\"VRAM: \", max_vram / (1024 ** 2))\n",
    "    \n",
    "    model.eval()\n",
    "    with gpytorch.settings.prior_mode():\n",
    "        output = (model(test_x))\n",
    "    means = output.mean.cpu()\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    mse_l_ski.append(MSE.item())\n",
    "    time_l_ski.append(uTime)\n",
    "    print('Test MAE: {}'.format(torch.mean((means.cpu() - test_y.cpu())*(means.cpu() - test_y.cpu()))))\n",
    "\n",
    "\n",
    "\n",
    "print(statistics.mean(mse_l_ski))\n",
    "print(statistics.stdev(mse_l_ski))\n",
    "\n",
    "print(statistics.mean(time_l_ski))\n",
    "print(statistics.stdev(time_l_ski))\n",
    "\n",
    "print(round(statistics.mean(mse_l_ski),5),round(statistics.stdev(mse_l_ski),5), round(statistics.mean(time_l_ski),5), round(statistics.stdev(time_l_ski),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3f645-92de-4bed-86da-6c77d005c177",
   "metadata": {},
   "source": [
    "# VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aecce4-4e59-48d5-a6ab-b89d65c77184",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch_size = 32\n",
    "smoke_test = False\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)#batch_size=1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=my_batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=my_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        print(1)\n",
    "\n",
    "        variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(m)\n",
    "\n",
    "        if gpu:\n",
    "            inducing_points = inducing_points.cuda()\n",
    "        print(2)\n",
    "\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        print(21)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        print(22)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        print(23)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=d))\n",
    "        print(3)\n",
    "        \n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        vram_usage()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "begin = time.time()\n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 256\n",
    "    training_batch_size = 64\n",
    "\n",
    "k = 160#320\n",
    "training_batch_size = 320*4\n",
    "\n",
    "mse_l_vnn = []\n",
    "time_l_vnn = []\n",
    "\n",
    "for i in np.arange(0,n_replicates):\n",
    "    print(\"Replicate: \",i)\n",
    "    mem_begin = get_mem()\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # Note: one should use full training set as inducing points!\n",
    "    model = GPModel(inducing_points=train_x[::1].contiguous(), likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "    \n",
    "    if gpu:\n",
    "        likelihood = likelihood.cuda()\n",
    "        model = model.cuda()\n",
    "    \n",
    "    print(time.time()-begin)\n",
    "    \n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    num_epochs = 1 if smoke_test else 20\n",
    "    num_epochs = 10#30\n",
    "    num_batches = model.variational_strategy._total_training_batches\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "    \n",
    "    # optimizer = torch.optim.Adam([\n",
    "    #     {'params': model.parameters()},\n",
    "    #     {'params': likelihood.parameters()},\n",
    "    # ], lr=0.05)\n",
    "    \n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    #mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    begin = time.time()\n",
    "    epochs_iter = tqdm.tqdm(range(num_epochs), desc=\"Epoch\", leave=True, position = 0)\n",
    "    for epoch in epochs_iter:\n",
    "        minibatch_iter = tqdm.tqdm(range(num_batches), leave=True, position = 0)\n",
    "    \n",
    "        for i in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x=None)\n",
    "            # Obtain the indices for mini-batch data\n",
    "            current_training_indices = model.variational_strategy.current_training_indices\n",
    "            # Obtain the y_batch using indices. It is important to keep the same order of train_x and train_y\n",
    "            y_batch = train_y[...,current_training_indices]\n",
    "            if gpu:\n",
    "                y_batch = y_batch.cuda()\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            vram_usage()\n",
    "            optimizer.step()\n",
    "    uTime = time.time() - begin\n",
    "    print(\"Time: \", time.time() - begin)\n",
    "    print(\"VRAM: \", max_vram/(1024 ** 2))\n",
    "    print(\"RAM: \", (get_mem() - mem_begin)/(1024**2))\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    MSE = torch.mean((means - test_y.cpu())*(means - test_y.cpu()))\n",
    "    #print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "    mse_l_vnn.append(MSE.item())\n",
    "    time_l_vnn.append(uTime)\n",
    "\n",
    "    model = None\n",
    "    likelihood = None\n",
    "    mll = None\n",
    "    optimizer = None\n",
    "    epochs_iter = None\n",
    "    if gpu:\n",
    "        gc.collect()\n",
    "    \n",
    "    print('Test MAE: {}'.format(torch.mean((means - test_y.cpu())*(means - test_y.cpu()))))\n",
    "\n",
    "\n",
    "\n",
    "print(statistics.mean(mse_l_vnn))\n",
    "print(statistics.stdev(mse_l_vnn))\n",
    "\n",
    "print(statistics.mean(time_l_vnn))\n",
    "print(statistics.stdev(time_l_vnn))\n",
    "\n",
    "print(round(statistics.mean(mse_l_vnn),5),round(statistics.stdev(mse_l_vnn),5), round(statistics.mean(time_l_vnn),5), round(statistics.stdev(time_l_vnn),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d76261-c467-43ea-a72b-e0ec5a412fe1",
   "metadata": {},
   "source": [
    "# Compile Table (MSE and Time only)\n",
    "\n",
    "SKI\n",
    "SGPR\n",
    "LOVE\n",
    "DKL\n",
    "SVGP-CI\n",
    "SVGP\n",
    "NGD\n",
    "VNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1b7aa",
   "metadata": {},
   "source": [
    "Reordering\n",
    "SVGP\n",
    "SVGP-CI\n",
    "VNN\n",
    "NGD\n",
    "DKL\n",
    "SGPR\n",
    "SKI\n",
    "LOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b5546-a5ea-4818-ba7d-37e7fdd2ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVGP    --- MSE:\",statistics.mean(mse_l_svgp), \"(\",statistics.stdev(mse_l_svgp),\")  Time:\", statistics.mean(time_l_svgp), \"(\",statistics.stdev(time_l_svgp),\")\")\n",
    "print(\"SVGP-CI --- MSE:\",statistics.mean(mse_l_svgpci), \"(\",statistics.stdev(mse_l_svgpci),\")  Time:\", statistics.mean(time_l_svgpci), \"(\",statistics.stdev(time_l_svgpci),\")\")\n",
    "print(\"VNN     --- MSE:\",statistics.mean(mse_l_vnn), \"(\",statistics.stdev(mse_l_vnn),\")  Time:\", statistics.mean(time_l_vnn), \"(\",statistics.stdev(time_l_vnn),\")\")\n",
    "print(\"NGD     --- MSE:\",statistics.mean(mse_l_ngd), \"(\",statistics.stdev(mse_l_ngd),\")  Time:\", statistics.mean(time_l_ngd), \"(\",statistics.stdev(time_l_ngd),\")\")\n",
    "print(\"DKL     --- MSE:\",statistics.mean(mse_l_dkl), \"(\",statistics.stdev(mse_l_dkl),\")  Time:\", statistics.mean(time_l_dkl), \"(\",statistics.stdev(time_l_dkl),\")\")\n",
    "print(\"SGPR    --- MSE:\",statistics.mean(mse_l_sgpr), \"(\",statistics.stdev(mse_l_sgpr),\")  Time:\", statistics.mean(time_l_sgpr), \"(\",statistics.stdev(time_l_sgpr),\")\")\n",
    "print(\"SKI     --- MSE:\",statistics.mean(mse_l_ski), \"(\",statistics.stdev(mse_l_ski),\")  Time:\", statistics.mean(time_l_ski), \"(\",statistics.stdev(time_l_ski),\")\")\n",
    "print(\"LOVE    --- MSE:\",statistics.mean(mse_l_love), \"(\",statistics.stdev(mse_l_love),\")  Time:\", statistics.mean(time_l_love), \"(\",statistics.stdev(time_l_love),\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaleGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
